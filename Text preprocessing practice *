# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]
#[('the', 150), ('of', 81), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('debugging', 40), ('for', 26), ('is', 25), ('or', 25)]
#Just capture alphabetical. It remove punctuations

# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]
#engish_stops is a list from stopwords.words('english')
#[('debugging', 40), ('system', 19), ('software', 16), ('tools', 14), ('process', 12), ('computer', 12), ('used', 12), ('bug', 11), ('http', 11), ('term', 11)]
#remove the word stopping like the, of, is, etc)

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()


# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]
#[('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]
#this removes plurality. systems to system

# Create the bag-of-words: bow
bow = Counter(lemmatized)

# Print the 10 most common tokens
print(bow.most_common(10))
